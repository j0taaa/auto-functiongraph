name: Build and Deploy FunctionGraph Containers

on:
  push:
    branches:
      - main
  workflow_dispatch:

env:
  HUAWEICLOUD_REGION: ${{ secrets.HUAWEICLOUD_REGION }}
  HUAWEICLOUD_PROJECT_ID: ${{ secrets.HUAWEICLOUD_PROJECT_ID }}
  HUAWEICLOUD_ACCESS_KEY: ${{ secrets.HUAWEICLOUD_ACCESS_KEY }}
  HUAWEICLOUD_SECRET_KEY: ${{ secrets.HUAWEICLOUD_SECRET_KEY }}
  HUAWEICLOUD_FUNCTION_AGENCY: ${{ secrets.HUAWEICLOUD_FUNCTION_AGENCY }}
  SWR_ORGANIZATION: ${{ secrets.SWR_ORGANIZATION }}
  SWR_NAMESPACE: ${{ secrets.SWR_NAMESPACE }}
  SWR_ENDPOINT: ${{ secrets.SWR_ENDPOINT }}
  OBS_ENDPOINT: https://obs.sa-brazil-1.myhuaweicloud.com
  OBS_STATE_BUCKET: tf-state-autofunctiongraph
  OBS_STATE_KEY: fgs/functions.tfstate

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    environment: autofg

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to SWR
        env:
          SWR_USERNAME: ${{ secrets.SWR_USERNAME }}
          SWR_PASSWORD: ${{ secrets.SWR_PASSWORD }}
          FALLBACK_USERNAME: ${{ env.HUAWEICLOUD_ACCESS_KEY }}
          FALLBACK_PASSWORD: ${{ env.HUAWEICLOUD_SECRET_KEY }}
          SWR_ENDPOINT: ${{ env.SWR_ENDPOINT }}
        run: |
          username="${SWR_USERNAME:-$FALLBACK_USERNAME}"
          password="${SWR_PASSWORD:-$FALLBACK_PASSWORD}"

          if [ -z "$username" ] || [ -z "$password" ]; then
            echo "SWR credentials are not configured. Provide SWR_USERNAME/SWR_PASSWORD secrets or reuse the Huawei Cloud access key and secret key." >&2
            exit 1
          fi

          if [ -z "$SWR_ENDPOINT" ]; then
            echo "SWR_ENDPOINT must be provided." >&2
            exit 1
          fi

          docker login "$SWR_ENDPOINT" -u "$username" --password-stdin <<<"$password"

      - name: Build and push function images
        run: |
          python <<'PY'
          import json
          import os
          import pathlib
          import subprocess

          projects_root = pathlib.Path("projects")
          if not projects_root.exists():
              raise SystemExit("No projects directory found")

          functions = {}
          commit = subprocess.check_output(["git", "rev-parse", "--short", "HEAD"], text=True).strip()
          endpoint = os.environ["SWR_ENDPOINT"].rstrip("/")
          organization = os.environ["SWR_ORGANIZATION"].strip("/")
          namespace = os.environ.get("SWR_NAMESPACE", "").strip("/")
          agency = os.environ.get("HUAWEICLOUD_FUNCTION_AGENCY", "").strip()

          if not agency:
              raise SystemExit("HUAWEICLOUD_FUNCTION_AGENCY environment variable must be set for Terraform inputs")

          for project in sorted(projects_root.iterdir()):
              if not project.is_dir():
                  continue

              name = project.name
              path_parts = [organization]
              if namespace:
                  path_parts.append(namespace)
              path_parts.append(name)

              image = f"{endpoint}/{'/'.join(path_parts)}:{commit}"

              subprocess.run(["docker", "build", "-t", image, str(project)], check=True)
              subprocess.run(["docker", "push", image], check=True)

              functions[name] = {
                  "name": name,
                  "description": f"Auto FunctionGraph deployment for {name}",
                  "handler": "-",
                  "memory_size": 256,
                  "timeout": 30,
                  "agency": agency,
                  "image_url": image,
              }

          terraform_dir = pathlib.Path("terraform")
          terraform_dir.mkdir(exist_ok=True)
          with open(terraform_dir / "functions.auto.tfvars.json", "w", encoding="utf-8") as fh:
              json.dump({"functions": functions}, fh)
          PY

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.6

      - name: Install AWS CLI for OBS state sync
        run: |
          sudo apt-get update
          sudo apt-get install -y awscli

      - name: Configure AWS CLI for OBS
        env:
          AWS_ACCESS_KEY_ID: ${{ env.HUAWEICLOUD_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ env.HUAWEICLOUD_SECRET_KEY }}
        run: |
          aws configure set default.region "${HUAWEICLOUD_REGION}"
          aws configure set default.s3.signature_version s3v4

      - name: Download Terraform state from OBS
        working-directory: terraform
        env:
          AWS_ACCESS_KEY_ID: ${{ env.HUAWEICLOUD_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ env.HUAWEICLOUD_SECRET_KEY }}
          AWS_DEFAULT_REGION: ${{ env.HUAWEICLOUD_REGION }}
        run: |
          set -euo pipefail
          aws --endpoint-url "${OBS_ENDPOINT}" \
            s3 cp "s3://${OBS_STATE_BUCKET}/${OBS_STATE_KEY}" terraform.tfstate \
            || echo "No remote Terraform state found; starting with a fresh state"

      - name: Terraform Init
        working-directory: terraform
        run: terraform init

      - name: Import existing functions into state
        working-directory: terraform
        env:
          TF_VAR_region: ${{ env.HUAWEICLOUD_REGION }}
          TF_VAR_project_id: ${{ env.HUAWEICLOUD_PROJECT_ID }}
          TF_VAR_access_key: ${{ env.HUAWEICLOUD_ACCESS_KEY }}
          TF_VAR_secret_key: ${{ env.HUAWEICLOUD_SECRET_KEY }}
        run: |
          set -euo pipefail
          python <<'PY'
          import json
          import os
          import pathlib
          import subprocess

          tfvars_path = pathlib.Path("functions.auto.tfvars.json")
          if not tfvars_path.exists():
              raise SystemExit("functions.auto.tfvars.json not found; build step must run before import")

          with tfvars_path.open("r", encoding="utf-8") as fh:
              data = json.load(fh)

          functions = data.get("functions", {})
          if not functions:
              raise SystemExit("No functions defined; nothing to import")

          region = os.environ.get("TF_VAR_region", "").strip()
          project_id = os.environ.get("TF_VAR_project_id", "").strip()

          existing_state = subprocess.run([
              "terraform",
              "state",
              "list",
          ], capture_output=True, text=True, check=False)

          tracked = set(line.strip() for line in existing_state.stdout.splitlines())

          for logical_name, config in sorted(functions.items()):
              addr = f'huaweicloud_fgs_function.function["{logical_name}"]'
              if addr in tracked:
                  continue

              function_name = config.get("name", logical_name)
              app = config.get("app", "default") or "default"

              candidates = [function_name]
              if region and project_id:
                  base = f"urn:fss:{region}:{project_id}:function:{app}:{function_name}"
                  candidates.append(base)
                  candidates.append(f"{base}:latest")

              imported = False
              for candidate in candidates:
                  result = subprocess.run([
                      "terraform",
                      "import",
                      addr,
                      candidate,
                  ], capture_output=True, text=True)

                  if result.returncode == 0:
                      print(f"Imported {addr} using identifier '{candidate}'")
                      imported = True
                      break

              if not imported:
                  print(
                      f"Warning: unable to import existing function for {addr}. "
                      "If the function already exists, please import it manually or remove it before rerunning."
                  )
          PY

      - name: Terraform Apply
        working-directory: terraform
        env:
          TF_VAR_region: ${{ env.HUAWEICLOUD_REGION }}
          TF_VAR_project_id: ${{ env.HUAWEICLOUD_PROJECT_ID }}
          TF_VAR_access_key: ${{ env.HUAWEICLOUD_ACCESS_KEY }}
          TF_VAR_secret_key: ${{ env.HUAWEICLOUD_SECRET_KEY }}
        run: terraform apply -auto-approve

      - name: Upload Terraform state to OBS
        if: always()
        working-directory: terraform
        env:
          AWS_ACCESS_KEY_ID: ${{ env.HUAWEICLOUD_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ env.HUAWEICLOUD_SECRET_KEY }}
          AWS_DEFAULT_REGION: ${{ env.HUAWEICLOUD_REGION }}
        run: |
          set -euo pipefail
          if [ -f terraform.tfstate ]; then
            aws --endpoint-url "${OBS_ENDPOINT}" \
              s3 cp terraform.tfstate "s3://${OBS_STATE_BUCKET}/${OBS_STATE_KEY}"
          else
            echo "No local Terraform state found to upload"
          fi
